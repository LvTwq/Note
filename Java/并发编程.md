[toc]

# 一、线程池

## 1、什么是线程池

我们会用各种池化技术来缓存创建昂贵的对象，比如线程池、连接池、内存池。一般是预先创建一些对象放入池中，使用的时候直接取出使用，用完归还以便复用，还会通过一定的策略调整池中缓存对象的数量，实现池的动态伸缩
由于线程的创建比较昂贵，随意、没有控制地创建大量线程会造成性能问题，因此短平快的任务一般考虑使用线程池来处理，而不是直接创建线程

## 2、Java 提供的线程池

Java 也提供了它⾃⼰实现的线程池模型—— ThreadPoolExecutor 。套⽤上⾯池化的想象来说，Java线程池就是为了最⼤化⾼并发带来的性能提升，并最⼩化⼿动创建线程的⻛险，将多个线程统⼀在⼀起管理的思想

```java
    public ThreadPoolExecutor(int corePoolSize,
                              int maximumPoolSize,
                              long keepAliveTime,
                              TimeUnit unit,
                              BlockingQueue<Runnable> workQueue,
                              ThreadFactory threadFactory,
                              RejectedExecutionHandler handler) {
        if (corePoolSize < 0 ||
            maximumPoolSize <= 0 ||
            maximumPoolSize < corePoolSize ||
            keepAliveTime < 0)
            throw new IllegalArgumentException();
        if (workQueue == null || threadFactory == null || handler == null)
            throw new NullPointerException();
        this.acc = System.getSecurityManager() == null ?
                null :
                AccessController.getContext();
        this.corePoolSize = corePoolSize;
        this.maximumPoolSize = maximumPoolSize;
        this.workQueue = workQueue;
        this.keepAliveTime = unit.toNanos(keepAliveTime);
        this.threadFactory = threadFactory;
        this.handler = handler;
    }
```

### 参数解析

| 序号 | 参数名          | 解释                                                                                                                     |
| ---- | --------------- | ------------------------------------------------------------------------------------------------------------------------ |
| 1    | corePoolSize    | 常驻核心线程数，如果大于0，即使任务执行完也不会销毁                                                                      |
| 2    | maximumPoolSize | 线程池能容纳可同时执行的最大线程数                                                                                       |
| 3    | keepAliveTime   | 线程池中线程空闲的时间，当空闲时间达到该值时，线程被销毁                                                                 |
| 4    | unit            | keepAliveTime 的时间单位，最终都会转换成【纳秒】                                                                         |
| 5    | workQueue       | 当请求的线程数大于corePoolSize时，线程进入该阻塞队列                                                                     |
| 6    | threadFactory   | 线程工厂，用来生产一组相同任务的线程，可以通过它增加前缀名                                                               |
| 7    | handler         | 执行拒绝策略，当workQueue达到上限，同时也达到maximumPoolSize就要通过这个来处理，比如拒绝、丢弃等，这是一种限流的保护措施 |

### 拒绝策略

* AbortPolicy：默认拒绝策略，会 throw RejectedExecution
* CallerRunsPolicy：提交任务的线程自己去执行该任务
* DiscardOldestPolicy：丢弃最⽼的任务，其实就是把最早进⼊⼯作队列的任务丢弃，然后把新任务加⼊到⼯作队列
* DiscardPolicy：相当⼤胆的策略，直接丢弃任务，没有任何异常抛出

## 3、禁⽌使⽤Executors创建线程池

### newFixedThreadPool

```java
/*
    翻看 newFixedThreadPool 方法的源码不难发现，线程池的工作队列直接 new 了一个 LinkedBlockingQueue，
    而默认构造方法的 LinkedBlockingQueue 是一个 Integer.MAX_VALUE 长度的队列，可以认为是无界的
    虽然使用 newFixedThreadPool 控制了固定线程数量，但任务队列是无界的，如果任务较多并且执行慢的话，队列可能会快速积压，撑爆内存
    */
public static ExecutorService newFixedThreadPool(int nThreads) {
    return new ThreadPoolExecutor(nThreads, nThreads,
                                    0L, TimeUnit.MILLISECONDS,
                                    new LinkedBlockingQueue<Runnable>());
}
```

传入的workQueue是一个边界为 **Integer.MAX_VALUE** 的队列，非常消耗内存

```java
    public LinkedBlockingQueue() {
        this(Integer.MAX_VALUE);
    }
```

并且该ThreadPoolExecutor方法使用的默认拒绝策略（直接拒绝），当很重要的请求过来直接拒绝显然不合适

```java
    public ThreadPoolExecutor(int corePoolSize,
                              int maximumPoolSize,
                              long keepAliveTime,
                              TimeUnit unit,
                              BlockingQueue<Runnable> workQueue) {
        this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,
             Executors.defaultThreadFactory(), defaultHandler);
    }
```

### newCachedThreadPool

```java
    public static ExecutorService newCachedThreadPool() {
        return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                      60L, TimeUnit.SECONDS,
                                      new SynchronousQueue<Runnable>());
    }
```

**这种线程池的最大线程数是 Integer.MAX_VALUE，可以认为是没有上限的，而其工作队列 SynchronousQueue 是一个没有存储空间的阻塞队列**，也就是说：
只要有任务过来，必须找到一条工作线程来处理，由于当前没有（核心线程数是0），所以一定会创建，可创建的最大线程数无上限，可能会撑爆内存
keepAliveTime 是60秒，也就是**在 60 秒之后所有的线程都是可以回收的**，导致不一定会撑爆

## 4、线程池默认的工作行为

1）不会初始化 corePoolSize 个线程，有任务来了才创建工作线程
2）当核心线程满了之后不会立即扩容线程池，而是把任务堆积到工作队列中
3）当工作队列满了后立刻扩容线程池，一直到线程个数达到 maximumPoolSize 为止
4）如果队列已满且达到了最大线程数后，还有任务进来，按照拒绝策略处理
5）当线程数大于核心线程数时，线程等待 keepAliveTime 后还是没有任务分配给他，这个线程会被回收（回收时，会对所谓的“核心线程”和“非核心线程”一视同仁，直到线程池中的线程数量等于设置的 corePoolSize）

![线程池工作流程](..\images\线程池工作流程.png)

## 5、线程池复用

线程池的意义在于复用，那这是不是意味着程序应该始终使用一个线程池呢？
当然不是。通过第一小节的学习我们知道，**要根据任务的“轻重缓急”来指定线程池的核心参数，包括线程数、回收策略和任务队列：**

* 对于执行比较慢、数量不大的IO任务，考虑更多的线程数，而不需要太大的队列
* 对于吞吐量较大的计算型任务，线程的数量不宜过多，可以是CPU核数或核数*2（理由是，线程一定调度到某个CPU进行执行，如果任务本身是CPU绑定的任务，那么过多的线程只会增加线程切换的开销，并不能提高吞吐量），但可能需要较长的队列来做缓冲

## 6、合理计算线程数量

一般多线程执行的任务类型可以分为 CPU 密集型和 I/O 密集型，根据不同的任务类型，计算线程数的方法也不一样

### 1）CPU 密集型

这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，多出来的这一个线程，是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，这时多出来的一个线程就可以充分利用 CPU 的空闲时间。

### 2）I/O 密集型

这种任务应用起来，系统会用大部分时间处理 I/O 交互，**而线程在处理 I/O 的时间段不会占用 CPU 来处理**，这时就可以将 CPU 交给其他线程使用，因此设置为 2N 是合理的

## 7、ForkJoinPool

**分治思想**：字⾯上的解释是「分⽽治之」，就是把⼀个复杂的问题分成两个
或更多的相同或相似的⼦问题，再把⼦问题分成更⼩的⼦问题……直到最后⼦问题可以简单的直接求解，原问题的
解就变成了⼦问题解的合并。
![](..\images\分治思想.png)

```java
public abstract class ForkJoinTask<V> implements Future<V>, Serializable
```

* fork(): 异步执行一个子任务（上面说的拆分）
* join(): 阻塞当前线程等待子任务的执行结果（上面说的合并）

在分治模型中，他还有两个抽象子类
![](..\images\ForkJoin1.png)

区别：有无返回值

```java
public abstract class RecursiveTask<V> extends ForkJoinTask<V> {
    protected abstract V compute();
}

public abstract class RecursiveAction extends ForkJoinTask<Void> {
    protected abstract void compute();
}
```

需要子类重写具体逻辑：

* 什么时候进一步拆分任务
* 什么时候满足最小执行任务（不再拆分）
* 什么时候汇总子任务结果

### 既然已经有了 ThreadPoolExecutor，为什么还要 ForkJoinPool

![](..\images\ForkJoin2.png)

* ThreadPoolExecutor 简单的并行操作主要是为了执行时间不确定的任务；
* 分治思想可以理解为一种父子依赖的关系，当依赖层级非常深，用 ThreadPoolExecutor 来处理这种关系不现实，所以 ForkJoinPool 作为功能补充就出现了
* ForkJoinPool 对于 n 并行度有 n 个独立队列，ThreadPoolExecutor 是共享队列，如果是大量耗时短的任务，ThreadPoolExecutor 的单队列会成为瓶颈

任务拆分后有依赖关系，还得减少线程之间的竞争，那就让线程执行属于自己的task，所以较 ThreadPoolExecutor 单个 TaskQueue 的形式，ForkJoinPool 是多个 TaskQueue 的形式:

![](..\images\ForkJoin3.png)

有多个任务队列，所以在ForkJoinPool 中就有一个数组形式的成员变量**WorkQueue[]**

> 任务队列有多个，提交的任务放到哪个队列中呢？

这就需要一套路由规则，提交的任务主要有两种：

* 有外部直接提交的（submission task）
* 也有任务自己fork出来的（worker task）

为了进一步区分这两种task，设计了一个简单的路由规则：

* 将 **submission task** 放到 WorkQueue 数组的 **「偶数」** 下标中
* 将 **worker task** 放在 WorkQueue 的 **「奇数」** 下标中，并且只有奇数下标才有线程( worker )与之相对

![](..\images\ForkJoin4.png)

每个任务的执行时间都不一样，执行快的线程的工作队列可能就是空的，为了最大化利用CPU资源，就允许从空闲线程拿**其它任务队列**中的内容，这个过程叫做**work-stealing（工作窃取）**

当前线程要执行一个任务，其他线程还有可能过来窃取任务，这就会产生竞争，为了减少竞争，WorkQueue 就设计成了一个双端队列：

* 支持LIFO(last-in-first-out)的push（放）和pop（拿）————**操作top端**
* 支持FIFO(first-in-first-out)的poll（拿）————**操作base端**

![](..\images\ForkJoin5.png)
线程（worker）操作自己的WorkQueue默认是**后进先出**（可选FIFO），当线程（worker）尝试窃取其它WorkQueue里的任务时，这个时候执行的是**先进先出**，即从base端获取，好处是：
1）**后进先出**操作只有**对应的worker**才能执行，push、pop 不需要考虑并发
2）拆分时，越大的任务越应该在WorkerQueue的base端，尽早分解，能尽快进入计算

### Java8 的 parallel()

* parallelStream 是一种并行流, 意思为处理任务时并行处理（前提是硬件支持, 如果单核 CPU, 只会存在并发处理, 而不会并行）
* 并行流底层就是使用的 ForkJoinPool, 一种**工作窃取算法线程池**
* ForkJoinPool 的优势在于, 可以充分利用多 CPU 的优势，把一个任务拆分成多个"小任务", 把多个"小任务"放到多个处理器核心上并行执行; 当多个"小任务"执行完成之后, 再将这些执行结果合并起来
* ForkJoinPool 实例内部线程总数 parallelism 默认为: 当前运行环境的 CPU 核数 - 1
* 并行流通过静态**makeCommonPool()**方法生成**ForkJoinPool**实例

```java
static final ForkJoinPool common;
/**
 * @return the common pool instance
 * @since 1.8
 */
public static ForkJoinPool commonPool() {
 // assert common != null : "static init error";
 return common;
}

// Common 是在静态块⾥⾯初始化的(只会被执⾏⼀次)：
common = java.security.AccessController.doPrivileged
 (new java.security.PrivilegedAction<ForkJoinPool>() {
 public ForkJoinPool run() { return makeCommonPool(); }});

private static ForkJoinPool makeCommonPool() {}
```

因为这个是一个单例通用的ForkJoinPool，所以：

> 如果使⽤通⽤ ForkJoinPool，最好只做 CPU 密集型的计算操作，不要有不确定性的 I/O 内容在任务⾥⾯，以防拖垮整体

## 8、线程池状态

![](..\images\线程池状态.png)

### RUNNING

此时能够接收新任务，以及对已添加的任务进行处理

### SHUTDOWN

调用线程池的shutdown()方法，线程池由RUNNING -> SHUTDOWN；
此时不再接收新任务，但能够处理已经添加的任务，当所有任务执行完毕，线程池被关闭

### STOP

调用线程池的shutdownNow()方法时，线程池由(RUNNING or SHUTDOWN ) -> STOP
此时不再接收新任务，不处理已添加的任务，并且会中断正在处理的任务

### TIDYING

当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -> TIDYING
当线程池在STOP状态，线程池执行的任务为空，就会由 STOP -> TIDYING

此时所有任务已终止，任务数量也为0，线程池变为 TIDYING 状态，执行钩子方法terminated()

### TERMINATED

线程池处在TIDYING状态时，执行完terminated()之后，就会由 TIDYING -> TERMINATED
此时线程池彻底终止

## 9、线程池实际使用

有一个统计页面要统计若干个指标，这些指标必须调外系统的接口，先去生成环境测了下接口的响应速度，最慢的需要4分钟。
我们肯定不能让用户等待这么久，而且用户看了页面没数据，也可能反复刷新页面，肯定会影响到我们自己的系统和外系统。
所以用了线程池做异步处理，首先我们建立了临时表，前端请求过来，直接返回数据库里的数据
往线程池中提交任务，每个查询指标对应一个任务，最后合并任务结果，把数据入库。
这样不会影响前端页面响应，数据的准确度虽然会有一定滞后性，但也是能够接受的。

## 10、异常处理

```java
@GetMapping("execute")
public void execute() throws InterruptedException {

    String prefix = "test";
    ExecutorService threadPool = Executors.newFixedThreadPool(1, new ThreadFactoryBuilder().setNameFormat(prefix+"%d").get());
    //提交10个任务到线程池处理，第5个任务会抛出运行时异常
    IntStream.rangeClosed(1, 10).forEach(i -> threadPool.execute(() -> {
        if (i == 5) throw new RuntimeException("error");
        log.info("I'm done : {}", i);
    }));

    threadPool.shutdown();
    threadPool.awaitTermination(1, TimeUnit.HOURS);
}
```

把任务提交给线程池处理，任务本身出现异常怎么办？

* 因为异常抛出，老线程退出了，线程池只能**重新创建一个线程**
* 由于没有手动捕获异常进行处理，ThreadGroup 帮我们进行了未捕获异常的默认处理，向标准错误输出打印了出现异常的线程名和异常信息，**这种没有以统一错误日志格式记录错误信息打印出来的形式，对生产级代码是不合适的**

可以在声明线程池时，自定义线程池的未捕获异常处理程序

```java
new ThreadFactoryBuilder()
  .setNameFormat(prefix+"%d")
  .setUncaughtExceptionHandler((thread, throwable)-> log.error("ThreadPool {} got exception", thread, throwable))
  .get()
```

通过线程池 ExecutorService 的 execute 方法提交任务到线程池处理，如果出现异常会导致线程退出，控制台输出中可以看到异常信息。那么，把 execute 方法改为 submit，线程就会退出，因为 FutureTask 在执行任务出现异常之后，异常存到了一个 outcome 字段中，只有在调用 get 方法获取 FutureTask 结果的时候，才会以 ExecutionException 的形式重新抛出异常。

# 二、Future 相关

## ExecutorService

![](..\images\future2.png)
可以看到，使⽤ExecutorService 的 execute() ⽅法依旧得不到返回值，⽽ submit() ⽅法清⼀⾊的返回
Future 类型的返回值

## FutureTask

![](..\images\future1.png)
Future 是一个接口，里面只有5个方法

```java
// 取消任务
boolean cancel(boolean mayInterruptIfRunning);
// 获取任务执⾏结果
V get() throws InterruptedException, ExecutionException;
// 获取任务执⾏结果，带有超时时间限制
V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException,
TimeoutException;
// 判断任务是否已经取消
boolean isCancelled();
// 判断任务是否已经结束
boolean isDone();
```

但 Future 是接⼝，其实使⽤ExecutorService.submit() ⽅法返回的⼀直都是 Future 的实现类 FutureTask
![](..\images\future3.png)

类结构：
![](..\images\future4.png)

很神奇的⼀个接⼝， FutureTask 实现了 RunnableFuture 接⼝，⽽ RunnableFuture 接⼝⼜同时继承了
Runnable 和 Future 接⼝，所以可以推断出 FutureTask 具有这两种接⼝的特性：

* 有 Runnable 特性，所以可以⽤在 ExecutorService 中配合线程池使⽤
* 有 Future 特性，所以可以从中获取到执⾏结果

**但是 FutureTask 实现的是 Runnable 接⼝，也就是只能重写 run() ⽅法，run() ⽅法⼜没有返回值，那么 run() ⽅法是如何将 call() ⽅法的返回结果和异常都保存起来的呢？**
其实⾮常简单, 就是通过 set(result) 保存正常程序运⾏结果，或通过 setException(ex) 保存程序异常信息

知道了将正常结果保存到了outcome变量里，那就需要知道FutureTask如何通过get()方法获取结果：

```java
public V get() throws InterruptedException, ExecutionException {
    int s = state;
    // 如果 state 还没到 set outcome 结果的时候，则调⽤ awaitDone() ⽅法阻塞⾃⼰
    if (s <= COMPLETING)
    s = awaitDone(false, 0L);
    // 返回结果
    return report(s);
}
```

## CompletableFuture

### Future 的短板

* 不能手动完成计算
  假如使用 Future 运行子线程调用远程接口来获取某些数据，对方服务器宕机了，如果想要手动结束计算，返回上次缓存中的价格，这是 Future 做不到的
* 调用 get() 方法会阻塞程序
  Future 不会通知你它完成了，它提供了一个 get() 方法，调用该方法会阻塞直到结果可用，没办法利用回调函数附加到 Future，并在 Future 的结果可用时调用它
* 不能链式执行
  烧水泡茶中，通过构造方法传参做到多个任务链式执行，万一有更多的任务，或者任务链的执行顺序有变，对原程序影响很大
* 整合多个 Future 执行结果方式笨重
  假设有多个 Future 并行执行，需要在这些任务全部执行完成之后做后续操作，Future本身是做不到的，需要借助 Executors 方法
* 没有异常处理

### 串行关系

runAsync：异步结算，没有返回值
supplyAsync：获取异步计算的返回结果
thenApply：通过回调方法获取结果，执行线程可能是异步线程，也可能是主线程
thenApplyAsync：有 async，说明是异步线程，会从ForkJoinPool.commonPool()中获取不同的线程进行执行
thenAccept：获取前序执⾏的结果，不从回调函数中返回任何结果（有入参，无返回值）
thenRun：无入参，无返回值
exceptionally：exceptionally 就相当于 catch，出现异常，将会跳过 thenApply 的后续操作，直接捕获异常，进⾏异常处理
handle：try/finally

### 聚合 And 关系

### 聚合 Or 关系

### 异常处理

# 三、并发容器

| 分类         | 名称                  | 特性                                                                                     | 适用场景                                                             |
| ------------ | --------------------- | ---------------------------------------------------------------------------------------- | -------------------------------------------------------------------- |
| Map并发容器  | Hashtable             | 强一致性                                                                                 | 对数据一致性有要求的场景                                             |
|              | ConcurrentHashMap     | 基于数据+链表+红黑树实现，CAS+Synchronized实现原子性，部分操作属于无锁操作，具有弱一致性 | 存取数据量小，查询操作频繁，且对数据没有强一致性要求的高并发场景     |
|              | ConcurrentSkipListMap | 基于跳跃表实现，具有弱一致性                                                             | 存取数据量大，增删改查操作频繁，且对数据没有强一致性要求的高并发场景 |
| List并发容器 | Vector                | 强一致性                                                                                 | 对数据强一致性有要求的场景                                           |
|              | CopyOnWriteArrayList  | 基于复制副本，用于有锁写操作，操作完成后，Array容器重新指向新的副本容器，具有弱一致性    | 读远大于写的场景                                                     |

## 1、ConcurrentHashMap

### 为什么Hashtable慢

使用了synchronized关键字对put等操作加锁，这是对整个对象加锁，也就是说在进行put等修改hash表的操作时，锁住了整个hash表，使得其表现得效率低下。

### ConcurrentHashMap - JDK 1.7

在JDK1.5~1.7版本，Java使用了分段锁机制实现ConcurrentHashMap。
简单讲，ConcurrentHashMap在对象中保存了一个**Segment数组**，即将整个Hash表划分为多个分段；
而每个Segment元素，即每个分段则类似于一个Hashtable；这样，在执行put操作时首先根据hash算法定位到元素属于哪个Segment，然后对该Segment加锁即可

#### 数据结构

Segment 代表“部分”、“一段”的意思，很多地方都将其描述为分段锁。
简单理解，ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全

![](..\images\java-thread-x-concurrent-hashmap-1.png)

**concurrencyLevel**: 并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是**16**，也就是说 ConcurrentHashMap 有 16 个 Segments，理论上最多可以支持16个线程并发写，只要他们分布在不同的Segment上。这个值可以在初始化时设置为其他值，但一旦初始化以后，不可扩容。

#### 初始化

* initialCapacity：初始容量，这个值指的是整个 ConcurrentHashMap 的初始容量，实际操作时需要平均分给每个Segment
* loadFactor：负载因子，Segment数组不可扩容，所以这个负载因子是给每个Segemt内部使用的

### ConcurrentHashMap - JDK 1.8

在JDK1.7之前，ConcurrentHashMap是通过分段锁机制来实现的，所以其最大并发度受Segment的个数限制。因此，在JDK1.8中，ConcurrentHashMap的实现原理摒弃了这种设计，而是选择了与HashMap类似的**数组+链表+红黑树**的方式实现，而加锁则采用**CAS和synchronized**实现

#### 数据结构

![](..\images\java-thread-x-concurrent-hashmap-2.png)

##### 初始化

### HashMap 为什么线程不安全

当两个线程同时插入元素，可能发生数据被覆盖的情况

```java
 if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);

```

当两个线程同时执行到以上代码时，发现没有发生哈希冲突，于是**新建Node节点**插入，这时新插入的节点会被后插入的节点覆盖，导致数据丢失

### 实现原理

# 四、synchronized

* 对象锁

包括**非静态方法**锁（默认锁对象为this，当前实例对象）和同步代码块锁

* 类锁

synchronized修饰**静态方法**或指定锁对象为Class对象

## 加锁释放锁的原理

使用 javap 命令反编译查看.class文件的信息

![](..\images\java-thread-x-key-schronized-x1.png)

`Monitorenter`和 `MonitorExit`指令，会让对象在执行时，使其锁计数器加1或者减1，每一个对象在同一时间只与一个monitor（锁）相关联，而一个monitor在同一时间只能被一个线程获得，一个对象在尝试获得与这个对象相关联的Monitor锁的所有权的时候，monitorenter指令会发生如下3种情况之一：

* monitor计数器为0，意味着目前还没有被获得，那这个线程就会立刻获得然后把锁计数器+1，一旦+1，别的线程再想获取，就需要等待
* 如果这个monitor已经拿到了这个锁的所有权，又重入了这把锁，那锁计数器就会累加，变成2，并且随着重入的次数，会一直累加
* 这把锁已经被别的线程获取了，等待锁释放

`monitorexit指令`：释放对于monitor的所有权，释放过程很简单，就是讲monitor的计数器减1，如果减完以后，计数器不是0，则代表刚才是重入进来的，当前线程还继续持有这把锁的所有权，如果计数器变成0，则代表当前线程不再拥有该monitor的所有权，即释放锁。

## 可重入原理：加锁计数器

允许一个线程二次请求自己持有的对象锁的临界资源，这种情况称为可重入锁
自己可以再次获取自己的内部锁。其实，可重入锁也支持在父子类继承的环境中。
当存在父子类继承关系时，子类是完全可以通过“可重入锁”调用父类的同步方法的。

线程不需要获取同一把锁

## 保证可见性原理：happens-before规则

对于会改变程序执⾏结果的重排序,JMM要求编译器和处理器必须禁⽌这种重排序。

如果A happens-before B，则A的执行结果对 B 可见，并且 A 的执行顺序优先于 B

## Synchronized 与 Lock

### synchronized的缺陷

* 效率低：只有在代码执行完毕或者抛出异常才会释放锁；试图获取锁的时候不能设置超时时间，不能中断一个正在使用锁的线程，相对而言，Lock可以中断和设置超时
* 不够灵活：加锁和释放的时机单一，每个锁只有一个单一的条件（某个对象）
* 无法知道是否能够成功获取到锁：Lock 可以拿到状态

### Lock解决相应问题

Lock类这里不做过多解释，主要看里面的4个方法:

- `lock()`: 加锁
- `unlock()`: 解锁
- `tryLock()`: 尝试获取锁，返回一个boolean值
- `tryLock(long,TimeUtil)`: 尝试获取锁，可以设置超时

## 父子类使用synchronized加锁

# 五、volatile

## 实现可见性

可见性问题主要指一个线程修改了共享变量值，而另一个线程却看不到。

引起可见性问题的主要原因是每个线程拥有自己的一个高速缓存区——线程工作内存

![](https://mmbiz.qpic.cn/mmbiz_png/Lj170PoHibScMk7rhSawq9uwe8JQxiagcfSpViaspAhs7zIRntnIaplnZfkjwA4qojUDR7dz2XmwH2iaQJUWlmnyRQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
jvms对ACC_VOLATILE的描述中写着，如果字节码中有用ACC_VOLATILE修饰的，代表这个变量不能被缓存。

当使用 volatile 修饰时，线程01对变量进行操作，会把变量变化的值强制刷新到主内存。
当线程02获取值时，会把自己内存里的属性值过期掉，再从**主内存**读取

## 保证原子性：单次读/写

volatile不能保证完全的原子性，只能保证单次的读/写操作具有原子性

### i++ 是原子性吗？

一个变量i被volatile修饰，两个线程想对这个变量修改，都对其进行自增操作，也就是 i++。
i++ 的过程可以分成三步：首先获取i的值，其次对i的值进行加1，最后将得到的新值写入到缓存中
1、线程A首先得到i的初始值100，但还没来得及修改，就阻塞了
2、线程B开始了，他也得到了i的值，此时i值还没被修改，所以他得到的也是100，然后对其进行加1操作，得到101，将新值写入缓存，再刷进主内存
3、根据可见性原则，这个值被其他线程可见
4、问题来了，线程A已经读取到了i的值为100，这个读取的原子操作已经结束了，所以这个可见性来的晚了，线程A阻塞结束，继续将这个值加1，得到101，所以不能保证原子性

## 有序性实现

happens-before 规则中有一条是 volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。

## synchronized 与 volatile

* 【进⼊】synchronized 块的内存语义是把在 synchronized 块内使⽤的变量从线程的⼯作内存中清除，从主内存 中读取
* 【退出】synchronized 块的内存语义是把在 synchronized 块内对共享变量的修改刷新到主内存中

当⼀个变量被声明为 volatile 时：

* 线程在【读取】共享变量时，会先清空本地内存变量值，再从主内存获取最新值
* 线程在【写⼊】共享变量时，不会把值缓存在寄存器或其他地⽅（就是刚刚说的所谓的「⼯作内存」），⽽是会把值刷新回主内存

synchronized 是独占锁/排他锁（就是有你没我的意思），同时只能有⼀个线程调⽤ add10KCount ⽅法，其他调 ⽤线程会被阻塞。所以三⾏ CPU 指令都是同⼀个线程执⾏完之后别的线程才能继续执⾏，这就是通常说说的 原⼦性 （线程执⾏多条指令不被中断）

 但 volatile 是⾮阻塞算法（也就是不排他），当遇到三⾏ CPU 指令⾃然就不能保证别的线程不插⾜了，这就是通 常所说的，volatile 能保证内存可⻅性，但是不能保证原⼦性

⼀句话，那什么时候才能⽤volatile关键字呢？

如果写⼊变量值不依赖变量当前值，那么就可以⽤ volatile

# 六、CAS、Unsafe类

## 什么是 CAS

Compare-And-Swap，直译就是对比交换。是一条CPU的原子指令，其作用是让CPU先进行比较两个值是否相等，然后**原子地**更新某个位置的值，经过调查发现，其实现方式是基于硬件平台的汇编指令，就是说CAS是靠硬件实现的，JVM只是封装了汇编调用，那些AtomicInteger类便是使用了这些封装后的接口。

简单解释：CAS操作需要输入两个数值，一个旧值(期望操作前的值)和一个新值，在操作期间先比较下在旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化则不交换。

CAS操作是原子性的，所以多线程并发使用CAS更新数据时，可以不使用锁。JDK中大量使用了CAS来更新数据而防止加锁(synchronized 重量级锁)来保持原子更新

## CAS 使用示例

不使用CAS，并发场景下，多线程同时修改一个变量的值，我们需要使用synchronized加锁

```java
private int i=0;
public synchronized int add(){
    return i++;
}
```

Java 中提供了 AtomicInteger 原子类，不需要加锁就能在多线程并发场景下实现数据一致性

```java
private  AtomicInteger i = new AtomicInteger(0);
public int add(){
    return i.addAndGet(1);
}
```

## CAS 问题

CAS 为乐观锁，synchronized 为悲观锁，因此 CAS 性能更佳，但也会有几个问题

### ABA 问题

CAS 检查值有没有发生变化，但如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时则会发现它的值没有发生变化，但是实际上却变化了

解决思路就是使用**版本号**，在变量前面追加版本号，每次变量更新的时候把版本号+1，那么A->B->A就会变成1A->2B->3A

JDK 提供了 AtomicStampedReference 来解决ABA问题。这个类主要维护包含一个对象引用以及一个可以自动更新的整数 `stamp`的对象来解决ABA问题。

compareAndSet方法的作用是首先检查**当前引用是否等于预期引用**，并且检查**当前标志是否等于预期标志**，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值

```java
public class AtomicStampedReference<V> {
    private static class Pair<T> {
        final T reference;  //维护对象引用
        final int stamp;  //用于标志版本
        private Pair(T reference, int stamp) {
            this.reference = reference;
            this.stamp = stamp;
        }
        static <T> Pair<T> of(T reference, int stamp) {
            return new Pair<T>(reference, stamp);
        }
    }
    private volatile Pair<V> pair;
    ....
  
    /**
      * expectedReference ：更新之前的原始值
      * newReference : 将要更新的新值
      * expectedStamp : 期待更新的标志版本
      * newStamp : 将要更新的标志版本
      */
    public boolean compareAndSet(V   expectedReference,
                             V   newReference,
                             int expectedStamp,
                             int newStamp) {
        // 获取当前的(元素值，版本号)对
        Pair<V> current = pair;
        return
            // 引用没变
            expectedReference == current.reference &&
            // 版本号没变
            expectedStamp == current.stamp &&
            // 新引用等于旧引用
            ((newReference == current.reference &&
            // 新版本号等于旧版本号
            newStamp == current.stamp) ||
            // 构造新的Pair对象并CAS更新
            casPair(current, Pair.of(newReference, newStamp)));
    }

    private boolean casPair(Pair<V> cmp, Pair<V> val) {
        // 调用Unsafe的compareAndSwapObject()方法CAS更新pair的引用为新引用
        return UNSAFE.compareAndSwapObject(this, pairOffset, cmp, val);
    }
```

* 首先，是用版本号控制
* 其次，不重复使用节点（Pair）的引用，每次都新建一个新的Pair来作为CAS比较的对象，而不是复用旧的
* 最后，外部传入元素值及版本号，而不是节点（Pair）的引用。

### 只能保证一个共享变量的原子操作

当对**一个共享变量**执行操作时，可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，就可以用锁。

还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i = 2，j = a，合并一下ij = 2a，然后用CAS来操作ij。

从Java 1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作

## UnSafe 类

Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类

![](..\images\java-thread-x-atomicinteger-unsafe.png)

### Unsafe 与 CAS

原子操作其实只支持下面三个方法

```java
public final native boolean compareAndSwapObject(Object paramObject1, long paramLong, Object paramObject2, Object paramObject3);

public final native boolean compareAndSwapInt(Object paramObject, long paramLong, int paramInt1, int paramInt2);

public final native boolean compareAndSwapLong(Object paramObject, long paramLong1, long paramLong2, long paramLong3);

```

## AtomicIntrger

```java
public class AtomicInteger extends Number implements java.io.Serializable {
    private static final Unsafe unsafe = Unsafe.getUnsafe();
    private static final long valueOffset;
    static {
        try {
            //用于获取value字段相对当前对象的“起始地址”的偏移量
            valueOffset = unsafe.objectFieldOffset(AtomicInteger.class.getDeclaredField("value"));
        } catch (Exception ex) { throw new Error(ex); }
    }

    private volatile int value;

    //返回当前值
    public final int get() {
        return value;
    }

    //递增加detla
    public final int getAndAdd(int delta) {
        //三个参数，1、当前的实例 2、value实例变量的偏移量 3、当前value要加上的数(value+delta)。
        return unsafe.getAndAddInt(this, valueOffset, delta);
    }

    //递增加1
    public final int incrementAndGet() {
        return unsafe.getAndAddInt(this, valueOffset, 1) + 1;
    }
...
}
```

可以看到使用的是volatile变量和CAS来进行更改数据的

* volatile 保证线程的可见性，多线程并发时，一个线程修改数据，可以保证其它线程立马看到修改后的值
* CAS保证数据更新的原子性

## 其他原子类

### 原子更新基本类型

使用原子的方式更新基本类型，Atomic包提供了以下3个类。

- AtomicBoolean: 原子更新布尔类型。
- AtomicInteger: 原子更新整型。
- AtomicLong: 原子更新长整型。

### 原子更新数组

通过原子的方式更新数组里的某个元素，Atomic包提供了以下的4个类：

- AtomicIntegerArray: 原子更新整型数组里的元素。
- AtomicLongArray: 原子更新长整型数组里的元素。
- AtomicReferenceArray: 原子更新引用类型数组里的元素。  这三个类的最常用的方法是如下两个方法：
- get(int index)：获取索引为index的元素值。
- compareAndSet(int i,E expect,E update): 如果当前值等于预期值，则以原子方式将数组位置i的元素设置为update值。

### 原子更新引用类型

Atomic包提供了以下三个类：

- AtomicReference: 原子更新引用类型。
- AtomicStampedReference: 原子更新引用类型, 内部使用Pair来存储元素值及其版本号。
- AtomicMarkableReferce: 原子更新带有标记位的引用类型。

这三个类提供的方法都差不多，首先构造一个引用对象，然后把引用对象set进Atomic类，然后调用compareAndSet等一些方法去进行原子操作，原理都是基于Unsafe实现，但AtomicReferenceFieldUpdater略有不同，更新的字段必须用volatile修饰。

# 七、ThreadLocal

通过**线程隔离**的方式防止任务在共享资源上产生冲突，可以为**每个线程**创建单独变量副本，避免因多线程操作共享变量而导致数据不一致的情况。

## 原理

```java
public T get() {
    Thread t = Thread.currentThread();
    ThreadLocalMap map = getMap(t);
    if (map != null) {
        ThreadLocalMap.Entry e = map.getEntry(this);
        if (e != null) {
            @SuppressWarnings("unchecked")
            T result = (T)e.value;
            return result;
        }
    }
    // 初始化threadLocals
    return setInitialValue();
}
```

* 首先获取当前线程对象t，从t中获取到ThreadLocalMap的成员属性threadLocals
* 如果当前线程的 threadLocals 已经初始化
  * 并且存在以当前ThreadLocal对象为key的值，则直接返回当前线程要获取的对象
  * 但是不存在以当前ThreadLocal对象为key的值，那就重新创建一个Connection对象，并且添加到当前线程的threadLocalsMap中，并返回
* 如果当前线程的threadLocals属性还没有被初始化，则重新创建一个ThreadLocalMap对象，并且创建一个Connection对象并添加到ThreadLocalMap对象中并返回

其实就是用了Map的数据结构给当前线程缓存了，要使用的时候就从本线程的threadLocals对象中获取，key就是当前线程，在当前线程下获取，肯定没有线程并发问题，能做到变量的线程间隔离

### ThreadLocalMap

* ThreadLocal 的静态内部类，
* 没有实现Map接口
* 仅仅用了一个Entry[]来存储key-value
* Entry 继承了 WeakReference

## 使用场景

如果各个线程之间对这个变量的访问没有依赖关系，即一个线程不关心其他线程是否对这个connect进行了修改，可以使用 ThreadLocal

比如数据库连接，有这么几种写法：

### 共享 connect

假如现在有一个连接管理类，定义了一个 connect 静态成员变量，打开/关闭数据库连接都不是同步方法，很可能出现一个线程连接数据库，一个线程要关闭数据库的情况

```java
public class ConnectionManager1 {

    /**
     * 到底需不需要将connect变量进行共享？
     */
    private static Connection connect = null;

    /**
     * 有线程安全问题
     * 1、有可能在 openConnection 中多次创建connect
     * 2、由于connect是共享变量，有可能一个线程使用connect进行数据库操作，另一个线程调用closeConnection关闭连接
     *
     * @return
     * @throws SQLException
     */
    public static Connection openConnection() throws SQLException {
        if (connect == null) {
            connect = DriverManager.getConnection("", "", "");
        }
        return connect;
    }

    public static void closeConnection() throws SQLException {
        if (connect != null) {
            connect.close();
        }
    }
}
```

### 每次使用都创建一个连接管理类

每次都是在方法内部创建的连接，那么线程之间自然不存在线程安全问题。但是这样会有一个致命的影响：导致服务器压力非常大，并且严重影响程序执行性能

```java
class ConnectionManager {
    private Connection connect = null;

    public Connection openConnection() {
        if (connect == null) {
            connect = DriverManager.getConnection();
        }
        return connect;
    }

    public void closeConnection() {
        if (connect != null)
            connect.close();
    }
}

class Dao {
    public void insert() {
        ConnectionManager connectionManager = new ConnectionManager();
        Connection connection = connectionManager.openConnection();

        // 使用connection进行操作

        connectionManager.closeConnection();
    }
}
```

### 使用ThreadLocal

```java
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;

public class ConnectionManager {

    private static final ThreadLocal<Connection> dbConnectionLocal = new ThreadLocal<Connection>() {
        @Override
        protected Connection initialValue() {
            try {
                return DriverManager.getConnection("", "", "");
            } catch (SQLException e) {
                e.printStackTrace();
            }
            return null;
        }
    };

    public Connection getConnection() {
        return dbConnectionLocal.get();
    }
}
```

那么这种情况下使用ThreadLocal是再适合不过的了，因为ThreadLocal在每个线程中对该变量会创建一个副本，即每个线程内部都会有一个该变量，且在线程内部任何地方都可以使用，线程之间互不影响，这样一来就不存在线程安全问题，也不会严重影响程序执行性能
但是要注意，虽然ThreadLocal能够解决上面说的问题，但是由于在每个线程中都创建了副本，所以要考虑它对资源的消耗，比如内存的占用会比不使用ThreadLocal要大

## 内存泄漏

如果使用**线程池**来操作ThreadLocal对象可能会造成内存泄漏，因为对于线程池里面不会销毁的线程，总会存在强引用

为了避免出现内存泄漏的情况，ThreadLocal提供了一个清除线程中对象的方法：remove()

# 八、线程生命周期

## 操作系统通用线程状态

![](..\images\image-20220327112018503.png)

* 初始状态：线程已经被创建，但是还没有分配CPU执行（**注意：这个创建其实是属于编程语言层面的，实际在操作系统中，真正的线程还没有被创建，比如Java中的 new Thread()**）
* 可运行状态：线程可以分配CPU执行，这时，**操作系统中的线程已经被创建成功了**
* 运行状态：操作系统会为**处在可运行状态的线程**分配CPU时间片，处在可运行状态的线程会变成运行状态
* 休眠状态：如果处在运行状态的线程调用**某个阻塞的API**或**等待某个事件条件可用**，线程会转换到休眠状态，**注意：此时线程会释放CPU使用权，休眠的线程永远没有机会获得CPU使用权，只有等待事件出现后，线程会从休眠状态转换到可运行状态**

## Java 线程状态

![](..\images\操作系统线程状态.png)

java 语言中

* 将通用线程状态的 `可运行状态`和 `运行状态`合并为Runnable
* 将 `休眠状态`细分为三种（**BLOCKED/WAITING/TIME_WAITING**）；这三种状态在操作系统眼里都是休眠状态，同样**不会获得CPU使用权**

除去线程生死，最重要的是RUNNABLE和休眠状态的转换

![img](..\images\)

### RUNNABLE ⬅➡ BLOCKED

只有一种情况会从 RUNNABLE ➡ BLOCKED：线程在等待 synchronized 内置锁

如果等待的线程获取到了 synchronized 内置锁，BLOCKED ➡ RUNNABLE

### RUNNABLE ⬅➡ WAITING

调用**不带时间参数的等待API**，就会从RUNNABLE进入到WAITING状态

当**被唤醒**就会从WAITING进入到RUNNABLE状态

### RUNNABLE ⬅➡  TIMED-WAITING

调用**带时间参数的等待API**，⾃然就从 RUNNABLE 状态进⼊ TIMED-WAITING 状态；当被唤醒或超时时间到就会从 TIMED_WAITING进⼊RUNNABLE状态

## 查看线程状态

### getState()

Thread 类中有 getState() 方法用于查看当前线程状态，该方法返回值就是上面的枚举类

### jstack

先查询到程序的pid，使用 jstack命令

## 其他常见问题

### sleep() 和 wait() 的区别

1、sleep 在**任何地方**都能使用；wait 方法必须放在**同步方法或者同步代码块**里，它们都需要处理 InterruptedException

2、sleep 是**让出CPU**（CPU可以去执行其他任务）指定时间后CPU再回到该线程继续往下执行，**不会释放获得的锁**；

使用wait，当前线程会释放锁，这是因为如果没有释放锁，那么其他线程无法进入对象的同步方法或同步代码块中，那么就无法执行notify()/notifyAll()方法来唤醒，造成死锁。

或者wait时间到，唤醒线程，才可以参与竞争

3、sleep是Thread的方法；wait是Object的方法

### notify() 和 notifyAll() 区别

notify()：

> 随机唤醒一个：一个共享变量上

notifyAll()：

> 唤醒所有：

### wait()和notify()为什么定义在Object中

Java提供的锁是对象级而不是线程级，线程获取了这个对象锁之后，**锁对象**要让当前线程释放锁资源，如果把wait()定义在Thread类，线程正在等待哪个锁就不明显了
简单讲，wait()和notify() 都是锁级别的操作，锁是对象，所以定义在Object中。

# 九、死锁

## 发生死锁的条件

* 互斥条件：一段时间内，某资源**只由一个线程占用**，如果此时还有其他线程请求资源，则请求者只能等待
* 请求和保持条件：线程已经保持了至少一个资源，但又提出了新的资源请求，并且该资源已经被其他线程占有，此时请求线程阻塞，但又对自己已获得的其他资源保持不放
* 不可剥夺条件：线程已获得的资源，在使用完之前，不能被剥夺
* 环路等待条件：A等待一个B占有的资源，B等待一个C占有的资源，C等待一个A占有的资源

其中「互斥条件」是并发编程的根基，这个条件没办法改变。但其他三个条件都有改变的可 能，也就是说破坏另外三个条件就不会出现上⾯说到的死锁问题

## 破坏请求和保持条件

一次性拿到所有资源，比如可以增加一个中间层，让中间层拿到所有资源，其他所有线程都向中间层请求资源，但这种方式并不好，如果业务量过大，对cpu的压力会太大

## 破坏不可剥夺条件

通知(notify/notifyall)和等待(wait)

## 破坏环路等待条件

将资源序号⼤⼩排序获取就会解决这个问题

# 十、等待通知机制

A想要拿到某一资源失败了，就不拿了（线程阻塞**⾃⼰** wait）

B把资源归还之后，主动通知A资源可⽤（通知等待的线程 notify/notifyAll）

## 使用的位置

wait(), notify()/notifyAll() 要在 synchronized 内部被使⽤，并且，如果锁的对象是this，就要 this.wait()，this.notify()/this.notifyAll() , 否则JVM就会抛出 java.lang.IllegalMonitorStateException 的。

因为等待/通知机制就是从【竞争】环境逐渐衍⽣ 出来的策略，不在锁竞争内部使⽤或等待/通知错了对象， ⾃然是不符合常理

## 使用while做判断

去等待队列中唤醒线程，再到再次尝试获取锁是有**时间差**的，当再次获取到锁时，线程曾经的要求不一定满足，需要重新进行条件判断

那为什么while就可以？

因为被唤醒的线程再次获取到锁之后是从原来的 wait 之后开始执⾏的，wait在循环⾥⾯，所以会再次进⼊循环条件重新进⾏条件判断。

## 尽量使⽤ notifyAll()

* notify()：

**随机**唤醒一个在该**共享变量**上调用wait()方法后被挂起的线程

* notifyAll()：

唤醒在该共享变量上由于调⽤wait() ⽅法⽽被挂起的所有线程

## 什么时候可以使用 notify()

1）所有等待线程拥有相同的等待条件

2）所有等待线程被唤醒后，执行相同的操作

3）只需要唤醒一个线程

# 十一、AQS

AbstractQueuedSynchronizer 用来构建**锁和同步器**的框架，ReentrantLock，Semaphore，ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS

## AQS 核心思想

如果被请求的**共享资源空闲**，则将当前请求资源的线程设置为有效的工作线程，并将共享资源设置为**锁定状态**。

如果被请求的共享资源**被占用**，那么就需要一套**线程阻塞等待**以及**被唤醒时锁分配的机制**，这个机制AQS是用CLH队列锁实现的——将暂时获取不到锁的线程加入到队列中。

> CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列(虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系)。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点(Node)来实现锁的分配

AQS使用一个int成员变量来表示**同步状态**，通过内置的FIFO队列来完成**获取资源线程的排队工作**，AQS使用CAS对该同步状态进行原子操作实现对其值的修改。

```java
private volatile int state;//共享变量，使用volatile修饰保证线程可见性
```

状态信息通过getState、setState、compareAndSetState进行操作

```java
//返回同步状态的当前值
protected final int getState() {  
        return state;
}
 // 设置同步状态的值
protected final void setState(int newState) { 
        state = newState;
}
//原子地(CAS操作)将同步状态值设置为给定值update如果当前同步状态的值等于expect(期望值)
protected final boolean compareAndSetState(int expect, int update) {
        return unsafe.compareAndSwapInt(this, stateOffset, expect, update);
}
```

## AQS 对资源的共享方式

* Exclusive（独占）：只有一个线程能执行，如ReentranLock。
  * 公平锁：按线程在队列中的**排队顺序**，先到者先拿锁
  * 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的
* Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock

ReentrantReadWriteLock 可以看成是组合式，因为ReentrantReadWriteLock也就是读写锁允许多个线程同时对某一资源进行读

## AQS底层使用了模板方法模式

使用者继承AbstractQueuedSynchronizer并重写指定的方法（主要是对共享资源state的获取和释放）将AQS组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法

### 同步器可重写的方法

![](..\images\AQS1.png)

表格⽅法描述中所说的 同步状态 就是上⽂提到的有 volatile 修饰的 state，所以我们在 重写 上⾯⼏个⽅法时，还 要通过同步器提供的下⾯三个⽅法（AQS 提供的）来获取或修改同步状态：

![](..\images\AQS2.png)

结构：
![](..\images\AQS3.png)

## AQS 实现分析

lock.lock() 这种阻塞式的锁是如何实现的？
有阻塞就需要有排队，实现排队必然需要队列，AQS中的队列是虚拟双向队列（FIFO）

队列中每个排队地个体就是一个Node

### Node 节点

AQS 内部维护了一个同步队列，用于管理同步状态

* 当线程**获取同步状态失败**时，就会将**当前线程以及等待状态等信息构造成一个Node节点**，将其加入到同步队列中尾部，**阻塞该线程**
* 当同步状态被释放时，会唤醒同步队列中 `首节点`的线程获取同步状态

![](..\images\AQS4.png)

乍一看有点乱，分类说明：
![](..\images\AQS5.png)

# 形而上的一些问题
